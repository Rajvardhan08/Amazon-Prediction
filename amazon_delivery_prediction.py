# -*- coding: utf-8 -*-
"""Amazon Delivery Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BeZU2yznoq8ZeO4fB6X-Xyv1wODW6Dvf
"""

# --- Step 1: Install and Import Necessary Libraries ---
# This command installs all the required Python packages for our analysis.
print("Installing libraries...")
!pip install pandas numpy matplotlib seaborn -q
print("Libraries installed successfully.")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files

# Set a style for our plots
sns.set_style('whitegrid')

# --- Step 2: Load the Dataset ---
# We use Google Colab's file uploader to easily get your dataset into the environment.
print("\n--- Data Loading ---")
print("Please upload your amazon_delivery.csv file using the button below.")
uploaded = files.upload()

# --- Step 3: Run the Analysis (only if a file was uploaded) ---
if uploaded:
    # Get the filename of the uploaded file
    file_name = next(iter(uploaded))
    print(f"\nSuccessfully uploaded '{file_name}'.")

    # Read the CSV file into a pandas DataFrame
    df = pd.read_csv(file_name)
    print("Dataset loaded successfully!")

    # --- Initial Data Inspection ---
    print("\n--- 2. Initial Data Inspection ---")
    # Clean column names immediately to remove any extra spaces
    df.columns = df.columns.str.strip()
    print("Column names have been cleaned.")

    print("\n--- Dataset Info ---")
    # Display a concise summary of the DataFrame, including data types and non-null values.
    df.info()

    print("\n--- Missing Values Check ---")
    # Check for and display the number of missing values in each column.
    print(df.isnull().sum())


    # --- 3. Data Visualization ---
    print("\n--- 3. Data Visualization ---")
    print("Generating plots...")

    # Visualization 1: Distribution of Delivery Time
    # This histogram shows how frequently different delivery times occur.
    plt.figure(figsize=(10, 6))
    sns.histplot(df['Delivery_Time'], bins=30, kde=True)
    plt.title('Distribution of Delivery Time (minutes)')
    plt.xlabel('Delivery Time (minutes)')
    plt.ylabel('Frequency')
    plt.show()

    # Visualization 2: Delivery Time by Traffic Condition
    # This box plot helps us see if there's a relationship between traffic and delivery time.
    # The 'order' parameter was removed to prevent errors if the dataset's categories don't match the hardcoded list.
    plt.figure(figsize=(12, 7))
    sns.boxplot(x='Traffic', y='Delivery_Time', data=df)
    plt.title('Delivery Time by Traffic Condition')
    plt.xlabel('Traffic Condition')
    plt.ylabel('Delivery Time (minutes)')
    plt.show()

    # Visualization 3: Delivery Time vs. Agent Rating
    # This scatter plot explores if there's a correlation between the delivery agent's rating and the time taken.
    plt.figure(figsize=(12, 7))
    sns.scatterplot(x='Agent_Rating', y='Delivery_Time', data=df, alpha=0.5)
    plt.title('Delivery Time vs. Agent Rating')
    plt.xlabel('Agent Rating')
    plt.ylabel('Delivery Time (minutes)')
    plt.show()

    print("\nExploratory Data Analysis complete.")

else:
    print("\nNo file was uploaded. Please run the cell again to upload your dataset.")

# --- Step 1: Install and Import Necessary Libraries ---
print("Installing libraries...")
!pip install pandas numpy scikit-learn haversine -q
print("Libraries installed successfully.")

import pandas as pd
import numpy as np
import joblib
from google.colab import files
from haversine import haversine, Unit
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings

warnings.filterwarnings('ignore')

# --- Step 2: Load the Dataset ---
print("\n--- Data Loading ---")
print("Please upload your amazon_delivery.csv file.")
uploaded = files.upload()

# --- Step 3: Preprocess Data and Train Models (only if a file was uploaded) ---
if uploaded:
    file_name = next(iter(uploaded))
    print(f"\nSuccessfully uploaded '{file_name}'.")
    df = pd.read_csv(file_name)
    print("Dataset loaded.")

    # --- 3.1: Data Cleaning and Feature Engineering ---
    print("\n--- Starting Data Preprocessing & Feature Engineering ---")

    # Clean column names
    df.columns = df.columns.str.strip()

    # Calculate distance using haversine formula
    df['Distance_km'] = df.apply(
        lambda row: haversine(
            (row['Store_Latitude'], row['Store_Longitude']),
            (row['Drop_Latitude'], row['Drop_Longitude']),
            unit=Unit.KILOMETERS
        ),
        axis=1
    )

    # Handle missing values before feature creation
    df['Agent_Age'].fillna(df['Agent_Age'].median(), inplace=True)
    df['Agent_Rating'].fillna(df['Agent_Rating'].median(), inplace=True)
    df['Delivery_Time'].fillna(df['Delivery_Time'].median(), inplace=True)

    # Define the list of columns you want to drop
    cols_to_drop = ['Order_ID', 'Agent_ID', 'Store_Latitude', 'Store_Longitude', 'Drop_Latitude', 'Drop_Longitude', 'Order_Date', 'Order_Time', 'Pickup_Time']

    # Create a list of columns that are actually present in the DataFrame
    cols_to_drop_existing = [col for col in cols_to_drop if col in df.columns]

    # Drop only the existing columns
    df_processed = df.drop(columns=cols_to_drop_existing)


    # One-Hot Encode categorical features
    df_processed = pd.get_dummies(df_processed, drop_first=True, dtype=int)

    print("Preprocessing and Feature Engineering complete.")

    # --- 3.2: Split Data for Training and Testing ---
    X = df_processed.drop('Delivery_Time', axis=1)
    y = df_processed['Delivery_Time']

    # Save column order for the Streamlit app
    joblib.dump(X.columns.tolist(), 'model_columns.pkl')

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    print(f"\nData split into {len(X_train)} training samples and {len(X_test)} testing samples.")

    # --- 3.3: Train and Evaluate Models ---
    models = {
        'Linear Regression': LinearRegression(),
        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
        'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
    }

    results = []
    best_model = None
    best_r2 = -1

    print("\n--- Training and Evaluating Models ---")
    for name, model in models.items():
        print(f"Training {name}...")
        model.fit(X_train, y_train)
        predictions = model.predict(X_test)

        # Calculate metrics
        rmse = np.sqrt(mean_squared_error(y_test, predictions))
        mae = mean_absolute_error(y_test, predictions)
        r2 = r2_score(y_test, predictions)

        results.append({'Model': name, 'RMSE': rmse, 'MAE': mae, 'R-squared': r2})

        # Track the best model based on R-squared score
        if r2 > best_r2:
            best_r2 = r2
            best_model = model

    # --- 3.4: Display Evaluation Metrics ---
    results_df = pd.DataFrame(results).sort_values(by='R-squared', ascending=False)
    print("\n--- Model Performance Evaluation ---")
    print(results_df.to_string(index=False))

    # --- 3.5: Save the Best Model ---
    if best_model:
        joblib.dump(best_model, 'delivery_time_model.pkl')
        print(f"\nBest model ({best_model.__class__.__name__}) saved successfully to 'delivery_time_model.pkl'")
        print("You can now download 'delivery_time_model.pkl' and 'model_columns.pkl' from the Colab file browser.")

else:
    print("\nNo file uploaded. Please run the cell again.")